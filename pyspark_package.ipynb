{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PySpark는 Spark를 위한 Python API이다.\n",
    "\n",
    "Public classes 다음이 있다.\n",
    "\n",
    "1. SparkContext - Spark 기능에 접근하기 위한 메인 입구\n",
    "2. RDD - Resilient Distributed Dataset, Spark의 기본 데이터\n",
    "3. Broadcast - 여러 작업에서 재사용되는 브로드캐스트 변수\n",
    "4. Accumulator - 값을 추가만 할 수 있는 sharable values\n",
    "5. SparkConf - 스파크 설정 클래스 \n",
    "6. SparkFiles - job과 함께 전달되는 파일 접근 클래스\n",
    "7. StorageLevel - 더 세분화된 캐시 지속 레벨\n",
    "8. TaskContext - 현재 실행되고 있는 task에 대한 정보 \n",
    "9. RDDBarrier - barrier execution mode를 실행할 때 RDD를 wrap하는 클래스\n",
    "10. BarrierTaskContext - barrier execution을 위한 extra 정보와 툴을 제공하는 TaskContext\n",
    "11. BarrierTaskInfo - Information about a barrioer task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스파크 애플리케이션을 위한 설정 다양한 스파크 파라미터를 키 밸류 쌍으로 설정하는데 사용\n",
    "# SparkConf()로 SparkConf를 생성할 수 있으며, spark Java system에서 value를 읽는다. \n",
    "# 어떠한 파라미터든 SparkConf object로 직접 설정할 수 있다.\n",
    "# 단위 테스트의 경우 SparkConf(false)를 호출하여 외부 설정을 건너 뛰고 시스템 속성에 상관없이 동일한 구성을 얻을 수 있다.\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "conf = pyspark.SparkConf().setAppName('appName').setMaster('local')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스파크 기능에 접근하기 위한 메인 entry point SparkContext는 스파크 클러스터에 대한 연결을 나타내고, RDD를 생성하고 클러스터에 변수를\n",
    "# broadcast할 수 있다.\n",
    "\n",
    "# JVM당 하나의 스파크콘텍스트만 활성화해야 합니다.새 스파크콘텍스트를 만들기 전에 활성 스파크콘텍스트를 중지해야 한다.\n",
    "# SparkContext 인스턴스는 기본적으로 여러 프로세스에서 공유하도록 지원되지 않으며 PySpark는 다중 프로세스 실행을 보장하지 않는다.  \n",
    "# 동시 처리 목적으로 스레드를 대신 사용해야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100, 200, 300, 400]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pyspark import SparkFiles\n",
    "path = 'test.txt'\n",
    "with open(path, \"w\") as testFile:\n",
    "    _ = testFile.write(\"100\")\n",
    "sc.addFile(path)\n",
    "\n",
    "def func(iterator):\n",
    "    with open(SparkFiles.get(\"test.txt\")) as testFile:\n",
    "        fileVal = int(testFile.readline())\n",
    "        return [x * fileVal for x in iterator]\n",
    "sc.parallelize([1, 2, 3, 4]).mapPartitions(func).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.range(5).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.range(2, 4).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.range(1, 7, 2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 16, 25]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD = sc.parallelize(range(6), 3)\n",
    "sc.runJob(myRDD, lambda part: [x * x for x in part], [0, 2], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "from time import sleep\n",
    "result = \"Not Set\"\n",
    "lock = threading.Lock()\n",
    "\n",
    "def map_func(x):\n",
    "    sleep(100)\n",
    "    raise Exception(\"Task should have been cancelled\")\n",
    "    \n",
    "def start_job(x):\n",
    "    global result\n",
    "    try:\n",
    "        sc.setJobGroup(\"job_to_cancel\", \"some description\")\n",
    "        result = sc.parallelize(range(x)).map(map_func).collect()\n",
    "    except Exception as e:\n",
    "        result = \"Cancelled\"\n",
    "    lock.release()\n",
    "def stop_job():\n",
    "    \n",
    "    sleep(5)\n",
    "    sc.cancelJobGroup(\"job_to_cancel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "suppress = lock.acquire()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\data\\spark\\spark-3.0.0-bin-hadoop2.7\\python\\pyspark\\util.py:141: UserWarning: Currently, 'setJobGroup' (set to local properties) with multiple threads does not properly work. \n",
      "Internally threads on PVM and JVM are not synced, and JVM thread can be reused for multiple threads on PVM, which fails to isolate local properties for each thread on PVM. \n",
      "To work around this, you can set PYSPARK_PIN_THREAD to true (see SPARK-22340). However, note that it cannot inherit the local properties from the parent thread although it isolates each thread on PVM and JVM with its own local properties. \n",
      "To work around this, you should manually copy and set the local properties from the parent thread to the child thread when you create another thread.\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "suppress = threading.Thread(target=start_job, args=(10,)).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "suppress = threading.Thread(target=stop_job).start()\n",
    "suppress = lock.acquire()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cancelled\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello world!', 'World!']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = os.path.join(\"sample-text.txt\")\n",
    "with open(path, \"w\") as testFile:\n",
    "   _ = testFile.write(\"Hello world!\")\n",
    "textFile = sc.textFile(path)\n",
    "textFile.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'World!']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = os.path.join(\"union-text.txt\")\n",
    "with open(path, \"w\") as testFile:\n",
    "   _ = testFile.write(\"Hello\")\n",
    "textFile = sc.textFile(path)\n",
    "textFile.collect()\n",
    "\n",
    "parallelized = sc.parallelize([\"World!\"])\n",
    "sorted(sc.union([textFile, parallelized]).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[WinError 183] 파일이 이미 있으므로 만들 수 없습니다: 'files'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-4b97c1439a93>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdirPath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"files\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirPath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirPath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"1.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"w\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileExistsError\u001b[0m: [WinError 183] 파일이 이미 있으므로 만들 수 없습니다: 'files'"
     ]
    }
   ],
   "source": [
    "# Small files are preferred, as each file will be loaded fully in memory.\n",
    "\n",
    "dirPath = os.path.join(\"files\")\n",
    "os.mkdir(dirPath)\n",
    "\n",
    "with open(os.path.join(dirPath, \"1.txt\"), \"w\") as file1:\n",
    "    _ = file1.write(\"1\")\n",
    "with open(os.path.join(dirPath, \"2.txt\"), \"w\") as file2:\n",
    "    _ = file2.write(\"2\")\n",
    "\n",
    "textFiles = sc.wholeTextFiles(dirPath)\n",
    "sorted(textFiles.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark의 기본 추상화인 RDD(Resilient Distributed Dataset).병렬로 작동할 수 있는 불변의 분할된 요소 집합을 나타냅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 14)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# aggregate the element of each partition\n",
    "\n",
    "seqOp = (lambda x, y: (x[0] + 1, x[0] + y))\n",
    "combOp = (lambda x, y: (0, x[1] + y[1] + x[0] + y[0]))\n",
    "\n",
    "sc.parallelize([1, 2, 3, 4, 5]).aggregate((0, 0), seqOp, combOp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 2)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqOp = (lambda x, y: (x[0] + y, x[1] + 1))\n",
    "combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "sc.parallelize([1, 2, 3, 4]).aggregate((0, 0), seqOp, combOp)\n",
    "\n",
    "sc.parallelize([12, 1]).aggregate((0, 0), seqOp, combOp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = sc.parallelize([(1, 2), (3, 4)]).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 2, 3: 4}\n"
     ]
    }
   ],
   "source": [
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([2, 3, 4, 2, 3, 4]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = sc.parallelize(range(1130)).map(str).countApproxDistinct()\n",
    "900 < n < 1200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'a': 2, 'b': 1})\n",
      "defaultdict(<class 'int'>, {1: 2, 2: 3})\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "print(rdd.countByKey())\n",
    "print(sc.parallelize([1, 2, 1, 2, 2], 2).countByValue())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "rdd.filter(lambda x: x % 2 == 0).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 2, 2, 3]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([2, 3, 4])\n",
    "sorted(rdd.flatMap(lambda x: range(1, x)).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 2), (2, 2), (3, 3), (3, 3), (4, 4), (4, 4)]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(rdd.flatMap(lambda x: [(x, x), (x, x)]).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 2), ('b', 1)]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "sorted(rdd.groupByKey().mapValues(len).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', [1, 1]), ('b', [1])]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(rdd.groupByKey().mapValues(tuple).collect())\n",
    "sorted(rdd.groupByKey().mapValues(list).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 2), ('b', 2), ('c', 2)]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(['a', 'b', 'c'])\n",
    "rdd.map(lambda x: (x, 2)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43.0"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([1.0, 5.0, 43.0, 10.0]).max()\n",
    "# rdd.collect()\n",
    "rdd\n",
    "# rdd.max()\n",
    "# rdd.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 2), ('b', 1)]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import add\n",
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "sorted(rdd.reduceByKey(add).collect())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
